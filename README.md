# The Issues

This project is an exploration of transformer networks as applied to Canadian parliamentary transcripts.

The objective is to create a model which can take some natural text (e.g. a paragraph),
and transform it into a vector in Euclidean space.
Texts that relate to similar issues should be close to one-another by some metric in that space.
This transformation can then be used to find related texts,
and even search for texts relating to a topic.

This is *not* an attempt at building a state-of-the art model.
That would require a very large model trained on a very large corpus with a very large processing cluster with some task-specific objective (e.g. STS).
Something like what was done in XLNet.

One of the motivations of this project, while not explicitly a goal,
is to learn about how easy it is to work with transformers,
and how suitable transformers are for working with similarity metrics in a dense space.

This project is using data from `https://openparliament.ca/`,
which is derived from `https://www.parl.gc.ca/HousePublications/Publication.aspx`.

## Examples

### Text generation

Before getting to deep into the technical details,
here are some parliamentary platitudes generated by the model:

> I will say that obviously we have to look at the measures that we have adopted. We have to take steps to ensure that the best possible quality of life is to come roaring back to work.

> Mr. Speaker, I am very pleased to see that we will continue to work with the provinces and territories. We are doing everything we have to get through the crisis and the provinces.

> However, we are committed to improving health and safety. We are committed to supporting it in a positive way of putting in place.

> Over one million Canadians expect the Prime Minister to be isolate for months in January, the Prime Minister said that the Prime Minister promised to comply with the quarantine requirements and they will not do so.

The model isn't nearly as convincing as its bigger counterparts (e.g. GPT).
There is a fair bit of repetition,
and it isn't always gramatically correct.
But it does a good job at capturing parliamentary themes and the (perhaps amusing) idiosyncrasies of parliamentary speak.

### Nearest neighbour

Here are a few texts and their nearest neighbour in the corpus:

Pair 1:

> We also know there is a light at the end of the tunnel. Canada has procured more doses of vaccine per capita than any other country in the world, and that light at the end of the tunnel should give us all hope. In the meantime we will work together in a team Canada approach to make sure that we get through the next several months together, with the health and safety of Canadians first and foremost.

> There is light at the end of the tunnel. Canada has secured solid commitments to vaccines, and every Canadian will have access to the vaccine, safely and for free, before the end of September.

Pair 2:

> When will the Liberals finally release a detailed plan to reopen our economy and get Canadians back to work?

> When will the Liberals release their reopening plan and economic recovery measures to protect future generations?

Pair 3:

> Madam Speaker, with Moraviantown and Caldwell First Nation in my riding, I want to begin by acknowledging the tragedy of 215 unmarked graves discovered at the Indian residential school in Kamloops, now the adopted home of my daughter and her family.

> Madam Speaker, on June 4, in front of Surrey City Hall, the south Asian community will hold a candlelight vigil to remember the 215 indigenous children whose remains were found in Kamloops on the grounds of Canadaâ€™s largest former residential school. The vigil is one of hundreds happening across the country to show solidarity with all Indigenous communities in Canada.

## Modeling approach

Transformers are neural networks that can efficiently process a sequence of input feature vectors.
They differ from RNNs and CNNs in that they can couple distant items in a sequence arbitrarily,
while still being able to exploit parameter-sharing.
In fact, without using positional encodings, they do not have a concept of distance along the sequence.
Rather, they learn their own similarity metric by which they gauge which two items should be related.
In the case of both CNNs and RNNs,
distant items are loosely coupled as they have to propagate through more layers (steps in the case or RNNs) to be combined into a single feature.

For simplicity (as this project is meant to be an *exploration*, not a definitive solution),
the Transformer will be trained as a simple auto-regressive language model.
No bells and whistles,
other than a small tweak described later to condition the model on the identity of the author (speaker) of a text.

### Tokenization

This model uses a tokenization model trained on the corpus.
The model in question is the "WordPiece" model,
as implemented in Huggingface's `tokenizer` package.
It is efficient,
has a good API,
and offers out-of-the box serialization and deserialization.

It makes sense to train a new tokenizer because the corpus is large and contains domain-specific language.
While pre-trained sub-word tokenizers (e.g. BERT) would probably cover the corpus quite well,
the choice of tokens and sub-word tokens wouldn't align as well to the data,
and the language model might need to spend more capacity learning how to stitch them together.

### Using the model to embed texts

The transformer language model
(as used in this code base)
maps each token in a sequence to some latent space.
That latent space encodes how likely it is for any word in the vocabulary to appear in the sequence.

The latent embedding of each token then contains information about topics in the sequence
(e.g. each token's latent embedding in a text about taxes will probably encode some probability for tokens relating to spending)
and grammatical rules 
(e.g. a verb token's latent embedding will probably encode a reduction in probability for other verbs as they shouldn't follow one-another).

There are many ways to use the transformer's latent token embeddings to create a single embedding for a sequence.
Some of these ideas are covered in <https://arxiv.org/pdf/1908.10084.pdf>.
But to keep things simple in this project,
a simple average over each token's latent embedding is used.

This can work well because the regions of the latent space which encode grammatical rules will tend to average out,
given that tokens in a sequence usually serve varied grammatical purposes.
By contrast,
the regions of the latent space which encode semantic (topical) information could be more coherent,
and thus could build up in the average.

### Conditional model

The project makes one small departure from a straight-forward language model training setting.
The sequences seen during learning are of the form:

`[SEP] ID [BOS] ... [EOS]`

Where the `[SEP]` token marks the separation of different sequences,
the `[BOS]` token marks the begging of a text,
the `[EOS]` token marks the end of a text,
and `ID` is the identifier for the author of the text
(in this case the parliamentarian who spoke the transcribed words).

The result is that the model can learn to condition token probabilities on the identity of an author.

When generating sequences
seeding the sequence with `[SEP] ID [BOS]` will result in a sequence in the style of the parliamentarian identified by `ID`.

### Evaluating the performance

In order to make choices about the model and fine-tune it,
it is important to have some metric by which its quality can be measured.
And this metric should be invariant of the hyper-parameters.

Using the final loss value in this setting isn't a great idea.
It can vary wildly when changing hyper-parameters
(e.g. it is easier to predict tokens in a smaller vocabulary)
with no perceived change in generation quality.

What is needed is a metric which more directly answers the question:
do the learned embeddings cluster similar topics.

In order to do this,
a few topic clusters are defined in the code base,
and the inter-cluster versus intra-cluster embedding distances are used to gauge the quality of the model.

Of course,
you could always change the learning task to be one which explicitly clusters these topics.
However the topics are too limited in scope and quality to serve this purpose.

## Requirements

You need Python 3.6 or later.
To build the database, you an installation of PostgreSQL.

## Code organization

The code is split up at a fairly granular level as it was mainly built for quick prototyping,
and changing many parameters from the command line.

The python module is found in `src/theissues`.
The scripts used to build the various resources are found in `scripts/`.

## Installation

To install the python package, from the project root run:

```bash
pip install .
```

### Building the data

```bash
scripts/download-db.sh
scripts/build-db-json.sh
```

## Training and evaluation

Prase the data, build the statements, and build the tokenizer:
```bash
python3 scripts/build-tokenizer.py \
    data/hansards.jsonl \
    data/statements.jsonl \
    data/tokenizer.json
```

Tokenize the statements, build the sequences:
```bash
python3 scripts/tokenize-statements.py \
    data/statements.jsonl \
    data/tokenizer.json \
    data/sequences.npy
```

Train the language model, build the model:
```bash
python3 scripts/train.py \
    data/sequences.npy \
    data/tokenizer.json \
    data/model/
```

Embed the sequences using the latent language model representation, build the embeddings:
```bash
python3 scripts/embed-sequences.py \
    data/sequences.npy \
    data/tokenizer.json \
    data/model/ \
    data/embeddings.npy
```

Evaluate how well the resulting embeddings can be clustered:
```bash
python3 scripts/evaluate-clusters.py \
    data/statements.jsonl \
    data/sequences.npy \
    data/embeddings.npy \
    data/tokenizer.json
```
